{"steps": {"columns": [{"type": "string", "name": "Step"}, {"type": "number", "name": "Kernel"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memcpy"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memset"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Runtime"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "DataLoader"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "CPU Exec"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Other"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}], "rows": [["5", 129520, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 159174us<br><b>Kernel: 129520us</b><br>Percentage: 81.37%</div>", 1948, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 159174us<br><b>Memcpy: 1948us</b><br>Percentage: 1.22%</div>", 90, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 159174us<br><b>Memset: 90us</b><br>Percentage: 0.06%</div>", 3346, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 159174us<br><b>Runtime: 3346us</b><br>Percentage: 2.1%</div>", 10482, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 159174us<br><b>DataLoader: 10482us</b><br>Percentage: 6.59%</div>", 12480, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 159174us<br><b>CPU Exec: 12480us</b><br>Percentage: 7.84%</div>", 1308, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 159174us<br><b>Other: 1308us</b><br>Percentage: 0.82%</div>"], ["6", 92970, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 150790us<br><b>Kernel: 92970us</b><br>Percentage: 61.66%</div>", 2436, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 150790us<br><b>Memcpy: 2436us</b><br>Percentage: 1.62%</div>", 63, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 150790us<br><b>Memset: 63us</b><br>Percentage: 0.04%</div>", 3144, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 150790us<br><b>Runtime: 3144us</b><br>Percentage: 2.09%</div>", 37456, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 150790us<br><b>DataLoader: 37456us</b><br>Percentage: 24.84%</div>", 13420, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 150790us<br><b>CPU Exec: 13420us</b><br>Percentage: 8.9%</div>", 1301, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 150790us<br><b>Other: 1301us</b><br>Percentage: 0.86%</div>"], ["7", 101813, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 141346us<br><b>Kernel: 101813us</b><br>Percentage: 72.03%</div>", 2111, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 141346us<br><b>Memcpy: 2111us</b><br>Percentage: 1.49%</div>", 70, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 141346us<br><b>Memset: 70us</b><br>Percentage: 0.05%</div>", 1756, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 141346us<br><b>Runtime: 1756us</b><br>Percentage: 1.24%</div>", 28987, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 141346us<br><b>DataLoader: 28987us</b><br>Percentage: 20.51%</div>", 5904, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 141346us<br><b>CPU Exec: 5904us</b><br>Percentage: 4.18%</div>", 705, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 141346us<br><b>Other: 705us</b><br>Percentage: 0.5%</div>"], ["8", 98964, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 160963us<br><b>Kernel: 98964us</b><br>Percentage: 61.48%</div>", 2078, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 160963us<br><b>Memcpy: 2078us</b><br>Percentage: 1.29%</div>", 68, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 160963us<br><b>Memset: 68us</b><br>Percentage: 0.04%</div>", 2040, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 160963us<br><b>Runtime: 2040us</b><br>Percentage: 1.27%</div>", 49981, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 160963us<br><b>DataLoader: 49981us</b><br>Percentage: 31.05%</div>", 7087, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 160963us<br><b>CPU Exec: 7087us</b><br>Percentage: 4.4%</div>", 745, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 160963us<br><b>Other: 745us</b><br>Percentage: 0.46%</div>"], ["9", 108112, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 148485us<br><b>Kernel: 108112us</b><br>Percentage: 72.81%</div>", 2072, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 148485us<br><b>Memcpy: 2072us</b><br>Percentage: 1.4%</div>", 74, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 148485us<br><b>Memset: 74us</b><br>Percentage: 0.05%</div>", 2926, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 148485us<br><b>Runtime: 2926us</b><br>Percentage: 1.97%</div>", 25411, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 148485us<br><b>DataLoader: 25411us</b><br>Percentage: 17.11%</div>", 9081, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 148485us<br><b>CPU Exec: 9081us</b><br>Percentage: 6.12%</div>", 809, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 148485us<br><b>Other: 809us</b><br>Percentage: 0.54%</div>"], ["10", 108661, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 167712us<br><b>Kernel: 108661us</b><br>Percentage: 64.79%</div>", 2089, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 167712us<br><b>Memcpy: 2089us</b><br>Percentage: 1.25%</div>", 74, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 167712us<br><b>Memset: 74us</b><br>Percentage: 0.04%</div>", 4174, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 167712us<br><b>Runtime: 4174us</b><br>Percentage: 2.49%</div>", 36220, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 167712us<br><b>DataLoader: 36220us</b><br>Percentage: 21.6%</div>", 14751, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 167712us<br><b>CPU Exec: 14751us</b><br>Percentage: 8.8%</div>", 1743, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 167712us<br><b>Other: 1743us</b><br>Percentage: 1.04%</div>"]]}, "performance": [{"name": "Average Step Time", "description": "", "value": 154745, "extra": 100, "children": [{"name": "Kernel", "description": "", "value": 106673, "extra": 68.93}, {"name": "Memcpy", "description": "", "value": 2122, "extra": 1.37}, {"name": "Memset", "description": "", "value": 73, "extra": 0.05}, {"name": "Runtime", "description": "", "value": 2898, "extra": 1.87}, {"name": "DataLoader", "description": "", "value": 31423, "extra": 20.31}, {"name": "CPU Exec", "description": "", "value": 10454, "extra": 6.76}, {"name": "Other", "description": "", "value": 1102, "extra": 0.71}]}], "recommendations": "<ul><li>This run has high time cost on input data loading. 20.3% of the step time is in DataLoader. You could try to set num_workers on DataLoader's construction and enable multi-processes on data loading. Reference: <a href =\"https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading\" target=\"_blank\">Single- and Multi-process Data Loading</a></li></ul>", "environments": [{"title": "Number of Worker(s)", "value": "1"}, {"title": "Device Type", "value": "GPU"}, {"title": "Number of Device(s)", "value": "1"}]}
{"device_total_time": {"title": "Device Total Time", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::cudnn_convolution_backward", 285514], ["CudnnConvolutionBackward", 285514], ["aten::cudnn_convolution_backward_weight", 149670], ["aten::cudnn_convolution_backward_input", 135844], ["aten::cudnn_convolution", 135735], ["aten::_convolution", 135735], ["aten::convolution", 135735], ["aten::conv2d", 135735], ["aten::cudnn_batch_norm_backward", 56884], ["CudnnBatchNormBackward", 56884], ["aten::cudnn_batch_norm", 33292], ["aten::_batch_norm_impl_index", 33292], ["aten::batch_norm", 33292], ["aten::threshold_backward", 26258], ["ReluBackward1", 26258], ["aten::add_", 23357], ["aten::threshold_", 17759], ["aten::relu_", 17759], ["aten::copy_", 12734], ["aten::to", 12734], ["aten::max_pool2d_with_indices_backward", 5046], ["MaxPool2DWithIndicesBackward", 5046], ["torch::autograd::AccumulateGrad", 2915], ["aten::fill_", 2414], ["aten::zero_", 2408], ["aten::mul_", 2380], ["aten::max_pool2d_with_indices", 1341], ["aten::max_pool2d", 1341], ["aten::zeros_like", 948], ["aten::add", 325], ["aten::mm", 295], ["AddmmBackward", 295], ["aten::mean", 256], ["aten::adaptive_avg_pool2d", 256], ["aten::addmm", 201], ["aten::div", 162], ["MeanBackward1", 162], ["aten::_log_softmax_backward_data", 64], ["LogSoftmaxBackward", 64], ["aten::_log_softmax", 60], ["aten::log_softmax", 60], ["aten::nll_loss_forward", 20], ["aten::nll_loss", 20], ["aten::nll_loss_backward", 18], ["NllLossBackward", 18], ["aten::ones_like", 6]]}, "device_self_time": {"title": "Device Self Time", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::cudnn_convolution_backward_weight", 149670], ["aten::cudnn_convolution_backward_input", 135844], ["aten::cudnn_convolution", 135735], ["aten::cudnn_batch_norm_backward", 56884], ["aten::cudnn_batch_norm", 33292], ["aten::threshold_backward", 26258], ["aten::add_", 23357], ["aten::threshold_", 17759], ["aten::copy_", 12734], ["aten::max_pool2d_with_indices_backward", 4098], ["aten::fill_", 2414], ["aten::mul_", 2380], ["aten::max_pool2d_with_indices", 1341], ["aten::add", 325], ["aten::mm", 295], ["aten::mean", 256], ["aten::addmm", 201], ["aten::div", 162], ["aten::_log_softmax_backward_data", 64], ["aten::_log_softmax", 60], ["aten::nll_loss_forward", 20], ["aten::nll_loss_backward", 18]]}, "host_total_time": {"title": "Host Total Time", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::add_", 96814], ["CudnnConvolutionBackward", 90857], ["aten::cudnn_convolution_backward", 87104], ["aten::conv2d", 61610], ["aten::copy_", 60140], ["aten::convolution", 57644], ["aten::batch_norm", 55154], ["aten::_convolution", 53789], ["aten::_batch_norm_impl_index", 51122], ["aten::cudnn_convolution", 49275], ["aten::cudnn_batch_norm", 47638], ["aten::to", 46057], ["aten::cudnn_convolution_backward_weight", 39006], ["aten::cudnn_convolution_backward_input", 38583], ["aten::mul_", 36843], ["aten::zero_", 36160], ["torch::autograd::AccumulateGrad", 34208], ["aten::empty", 33098], ["aten::stack", 33058], ["CudnnBatchNormBackward", 32186], ["aten::cat", 31169], ["aten::_cat", 30970], ["aten::div", 30671], ["aten::cudnn_batch_norm_backward", 27883], ["aten::contiguous", 24479], ["aten::fill_", 21081], ["aten::relu_", 16620], ["ReluBackward1", 15142], ["aten::add", 14945], ["aten::threshold_backward", 12601], ["aten::threshold_", 9128], ["aten::empty_like", 8255], ["aten::view", 4811], ["aten::resize_", 3415], ["aten::permute", 3161], ["aten::set_", 2994], ["aten::empty_strided", 1725], ["AddmmBackward", 1462], ["aten::unsqueeze", 1293], ["aten::addmm", 1274], ["aten::as_strided", 948], ["aten::mm", 847], ["MaxPool2DWithIndicesBackward", 763], ["aten::max_pool2d", 732], ["NllLossBackward", 719], ["aten::max_pool2d_with_indices_backward", 686], ["aten::t", 664], ["aten::zeros", 651], ["aten::max_pool2d_with_indices", 644], ["MeanBackward1", 606], ["aten::nll_loss_backward", 590], ["aten::adaptive_avg_pool2d", 566], ["aten::log_softmax", 527], ["aten::nll_loss", 500], ["aten::mean", 484], ["LogSoftmaxBackward", 451], ["aten::_log_softmax", 447], ["aten::nll_loss_forward", 425], ["aten::ones_like", 410], ["aten::_log_softmax_backward_data", 357], ["aten::zeros_like", 339], ["aten::transpose", 309], ["AddBackward0", 309], ["aten::reshape", 228], ["aten::flatten", 206], ["aten::expand", 141], ["TBackward", 140], ["ViewBackward", 121], ["aten::narrow", 87], ["aten::detach_", 64], ["aten::resize_as_", 54], ["aten::slice", 52], ["aten::conj", 46], ["detach_", 33]]}, "host_self_time": {"title": "Host Self Time", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::add_", 96814], ["aten::copy_", 60140], ["aten::cudnn_convolution", 43346], ["aten::mul_", 36843], ["aten::cudnn_convolution_backward_weight", 34576], ["aten::cudnn_convolution_backward_input", 34111], ["aten::cudnn_batch_norm", 33376], ["aten::empty", 33098], ["aten::_cat", 30756], ["aten::div", 26028], ["aten::fill_", 21081], ["aten::cudnn_batch_norm_backward", 20739], ["aten::zero_", 15259], ["aten::add", 14945], ["aten::threshold_backward", 10655], ["aten::cudnn_convolution_backward", 9515], ["aten::threshold_", 9128], ["aten::relu_", 7492], ["torch::autograd::AccumulateGrad", 6798], ["aten::view", 4811], ["aten::to", 4680], ["aten::_convolution", 4514], ["aten::empty_like", 4430], ["CudnnBatchNormBackward", 4303], ["aten::batch_norm", 4032], ["aten::conv2d", 3966], ["aten::convolution", 3855], ["CudnnConvolutionBackward", 3753], ["aten::_batch_norm_impl_index", 3484], ["aten::resize_", 3415], ["aten::set_", 2994], ["aten::permute", 2703], ["ReluBackward1", 2541], ["aten::contiguous", 2069], ["aten::empty_strided", 1725], ["aten::addmm", 1138], ["aten::as_strided", 948], ["aten::unsqueeze", 925], ["aten::mm", 771], ["aten::stack", 596], ["aten::nll_loss_backward", 590], ["aten::max_pool2d_with_indices", 497], ["aten::zeros", 463], ["aten::mean", 431], ["aten::nll_loss_forward", 425], ["aten::t", 355], ["aten::_log_softmax", 343], ["AddBackward0", 309], ["aten::max_pool2d_with_indices_backward", 293], ["aten::_log_softmax_backward_data", 246], ["aten::transpose", 226], ["aten::cat", 199], ["AddmmBackward", 195], ["NllLossBackward", 129], ["aten::expand", 113], ["MeanBackward1", 103], ["aten::ones_like", 100], ["LogSoftmaxBackward", 94], ["aten::max_pool2d", 88], ["aten::adaptive_avg_pool2d", 82], ["aten::log_softmax", 80], ["MaxPool2DWithIndicesBackward", 77], ["aten::nll_loss", 75], ["aten::reshape", 71], ["aten::flatten", 65], ["aten::zeros_like", 53], ["aten::conj", 46], ["aten::resize_as_", 44], ["aten::slice", 41], ["aten::narrow", 35], ["ViewBackward", 34], ["detach_", 33], ["aten::detach_", 31], ["TBackward", 29]]}}
{"data": {"columns": [{"type": "string", "name": "Name"}, {"type": "number", "name": "Calls"}, {"type": "number", "name": "Device Self Duration (us)"}, {"type": "number", "name": "Device Total Duration (us)"}, {"type": "number", "name": "Host Self Duration (us)"}, {"type": "number", "name": "Host Total Duration (us)"}], "rows": [["aten::cudnn_convolution_backward_weight", 318, 149670, 149670, 34576, 39006], ["aten::cudnn_convolution_backward_input", 312, 135844, 135844, 34111, 38583], ["aten::cudnn_convolution", 318, 135735, 135735, 43346, 49275], ["aten::cudnn_batch_norm_backward", 318, 56884, 56884, 20739, 27883], ["aten::cudnn_batch_norm", 318, 33292, 33292, 33376, 47638], ["aten::threshold_backward", 294, 26258, 26258, 10655, 12601], ["aten::add_", 2994, 23357, 23357, 96814, 96814], ["aten::threshold_", 294, 17759, 17759, 9128, 9128], ["aten::copy_", 588, 12734, 12734, 60140, 60140], ["aten::max_pool2d_with_indices_backward", 6, 4098, 5046, 293, 686], ["aten::fill_", 978, 2414, 2414, 21081, 21081], ["aten::mul_", 966, 2380, 2380, 36843, 36843], ["aten::max_pool2d_with_indices", 6, 1341, 1341, 497, 644], ["aten::add", 318, 325, 325, 14945, 14945], ["aten::mm", 12, 295, 295, 771, 847], ["aten::mean", 6, 256, 256, 431, 484], ["aten::addmm", 6, 201, 201, 1138, 1274], ["aten::div", 198, 162, 162, 26028, 30671], ["aten::_log_softmax_backward_data", 6, 64, 64, 246, 357], ["aten::_log_softmax", 6, 60, 60, 343, 447], ["aten::nll_loss_forward", 6, 20, 20, 425, 425], ["aten::nll_loss_backward", 6, 18, 18, 590, 590], ["aten::empty", 5748, 0, 0, 33098, 33098], ["aten::zero_", 996, 0, 2408, 15259, 36160], ["aten::zeros", 24, 0, 0, 463, 651], ["aten::set_", 192, 0, 0, 2994, 2994], ["aten::view", 840, 0, 0, 4811, 4811], ["aten::as_strided", 432, 0, 0, 948, 948], ["aten::permute", 192, 0, 0, 2703, 3161], ["aten::empty_like", 534, 0, 0, 4430, 8255], ["aten::contiguous", 192, 0, 0, 2069, 24479], ["aten::empty_strided", 402, 0, 0, 1725, 1725], ["aten::to", 408, 0, 12734, 4680, 46057], ["aten::unsqueeze", 192, 0, 0, 925, 1293], ["aten::resize_", 1926, 0, 0, 3415, 3415], ["aten::slice", 6, 0, 0, 41, 52], ["aten::narrow", 6, 0, 0, 35, 87], ["aten::_cat", 6, 0, 0, 30756, 30970], ["aten::cat", 6, 0, 0, 199, 31169], ["aten::stack", 6, 0, 0, 596, 33058], ["detach_", 6, 0, 0, 33, 33], ["aten::detach_", 6, 0, 0, 31, 64], ["aten::_convolution", 318, 0, 135735, 4514, 53789], ["aten::convolution", 318, 0, 135735, 3855, 57644], ["aten::conv2d", 318, 0, 135735, 3966, 61610], ["aten::_batch_norm_impl_index", 318, 0, 33292, 3484, 51122], ["aten::batch_norm", 318, 0, 33292, 4032, 55154], ["aten::relu_", 294, 0, 17759, 7492, 16620], ["aten::max_pool2d", 6, 0, 1341, 88, 732], ["aten::adaptive_avg_pool2d", 6, 0, 256, 82, 566], ["aten::reshape", 12, 0, 0, 71, 228], ["aten::flatten", 6, 0, 0, 65, 206], ["aten::transpose", 30, 0, 0, 226, 309], ["aten::t", 30, 0, 0, 355, 664], ["aten::expand", 12, 0, 0, 113, 141], ["aten::log_softmax", 6, 0, 60, 80, 527], ["aten::nll_loss", 6, 0, 20, 75, 500], ["aten::ones_like", 6, 0, 6, 100, 410], ["NllLossBackward", 6, 0, 18, 129, 719], ["LogSoftmaxBackward", 6, 0, 64, 94, 451], ["aten::conj", 12, 0, 0, 46, 46], ["AddmmBackward", 6, 0, 295, 195, 1462], ["torch::autograd::AccumulateGrad", 966, 0, 2915, 6798, 34208], ["TBackward", 6, 0, 0, 29, 140], ["ViewBackward", 6, 0, 0, 34, 121], ["MeanBackward1", 6, 0, 162, 103, 606], ["ReluBackward1", 294, 0, 26258, 2541, 15142], ["AddBackward0", 96, 0, 0, 309, 309], ["CudnnBatchNormBackward", 318, 0, 56884, 4303, 32186], ["aten::cudnn_convolution_backward", 318, 0, 285514, 9515, 87104], ["CudnnConvolutionBackward", 318, 0, 285514, 3753, 90857], ["aten::zeros_like", 6, 0, 948, 53, 339], ["aten::resize_as_", 6, 0, 0, 44, 54], ["MaxPool2DWithIndicesBackward", 6, 0, 5046, 77, 763]]}}
{"data": {"columns": [{"type": "string", "name": "Name"}, {"type": "number", "name": "Calls"}, {"type": "number", "name": "Total Duration (us)"}, {"type": "number", "name": "Mean Duration (us)"}, {"type": "number", "name": "Max Duration (us)"}, {"type": "number", "name": "Min Duration (us)"}], "rows": [["void cudnn::detail::dgrad_engine<float, 512, 6, 5, 3, 3, 3, false>(int, int, int, float const*, int, float const*, int, float*, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)", 167, 86835, 520, 1084, 330], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", 287, 61395, 214, 799, 43], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 6, 7, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 104, 53577, 515, 815, 393], ["void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)", 609, 47050, 77, 364, 6], ["void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)", 3489, 41190, 12, 364, 1], ["void implicit_convolve_sgemm<float, float, 1024, 6, 7, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 90, 40341, 448, 753, 381], ["void implicit_convolve_sgemm<float, float, 128, 6, 7, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 60, 28063, 468, 851, 361], ["volta_scudnn_128x128_stridedB_splitK_medium_nn_v1", 72, 27624, 384, 667, 354], ["volta_scudnn_128x64_stridedB_splitK_xregs_large_nn_v1", 34, 27184, 800, 885, 664], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, 512, true, 1>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", 150, 26234, 175, 426, 50], ["volta_sgemm_128x64_nt", 126, 23737, 188, 206, 155], ["volta_scudnn_128x128_stridedB_splitK_small_nn_v1", 48, 21753, 453, 705, 328], ["volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1", 39, 14259, 366, 370, 361], ["volta_sgemm_128x64_nn", 60, 11407, 190, 207, 156], ["volta_scudnn_128x64_stridedB_interior_nn_v1", 34, 10904, 321, 525, 263], ["void implicit_convolve_sgemm<float, float, 512, 6, 8, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 12, 8816, 735, 784, 660], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 5, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 14, 8346, 596, 990, 207], ["volta_scudnn_128x64_relu_interior_nn_v1", 24, 7210, 300, 311, 295], ["void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float, cudnn::reduced_divisor, int, cudnn::reduced_divisor, cudnn::bnFwPersistentState*, int, float, float, float, int, float, float, cudnnStatus_t*, bool)", 168, 7058, 42, 87, 14], ["volta_scudnn_128x128_stridedB_interior_nn_v1", 21, 5717, 272, 275, 269], ["void cudnn::ops::scalePackedTensor_kernel<float, float>(cudnnTensor4dStruct, float*, float)", 167, 5482, 33, 158, 6], ["void implicit_convolve_sgemm<float, float, 128, 5, 5, 3, 3, 3, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 12, 5341, 445, 449, 442], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 6, 8, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 7, 5298, 757, 780, 732], ["void cudnn::winograd_nonfused::winogradForwardOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradOutputParams<float, float>)", 123, 5250, 43, 68, 19], ["void explicit_convolve_sgemm<float, int, 1024, 5, 5, 3, 3, 3, 0, false>(int, int, int, float const*, int, float const*, int, float*, kernel_conv_params, unsigned long long, int, unsigned long long, int, float, float, int, float const*, float const*)", 6, 4895, 816, 822, 809], ["void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(int, float const*, long const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*)", 7, 4781, 683, 684, 682], ["void cudnn::winograd_nonfused::winogradForwardData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 123, 4765, 39, 63, 17], ["volta_scudnn_128x128_stridedB_medium_nn_v1", 14, 4312, 308, 325, 299], ["volta_scudnn_128x64_relu_medium_nn_v1", 6, 3829, 638, 641, 637], ["volta_scudnn_128x128_stridedB_small_nn_v1", 7, 3709, 530, 533, 527], ["void explicit_convolve_sgemm<float, int, 128, 6, 7, 3, 3, 5, 0, false>(int, int, int, float const*, int, float const*, int, float*, kernel_conv_params, unsigned long long, int, unsigned long long, int, float, float, int, float const*, float const*)", 6, 3533, 589, 659, 573], ["volta_scudnn_128x64_relu_xregs_large_nn_v1", 6, 3532, 589, 672, 569], ["volta_scudnn_128x64_relu_small_nn_v1", 12, 3395, 283, 296, 270], ["void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)", 1127, 2779, 2, 24, 1], ["void cudnn::winograd_nonfused::winogradForwardFilter4x4<float, float>(cudnn::winograd_nonfused::WinogradFilterParams<float, float>)", 123, 2617, 21, 66, 4], ["void cudnn::winograd_nonfused::winogradWgradData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 63, 2602, 41, 60, 21], ["void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)", 979, 2572, 3, 158, 0], ["void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, float>(cudnn::winograd_nonfused::WinogradDeltaParams<float, float>)", 63, 2449, 39, 61, 16], ["void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float, cudnn::reduced_divisor, int, cudnn::reduced_divisor, cudnn::bnBwPersistentState*, int, float, float, float, int, float, cudnnStatus_t*, bool)", 54, 2283, 42, 74, 19], ["void cudnn::cnn::im2col4d_kernel<float, long>(cudnn::cnn::im2col4d_params, cudnnConvolutionStruct, cudnnTensor4dStruct, float const*, float*)", 12, 1699, 142, 184, 98], ["void at::native::(anonymous namespace)::max_pool_forward_nchw<float, float>(int, float const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*, long*)", 6, 1341, 224, 224, 223], ["void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradWgradOutputParams<float, float>)", 63, 1306, 21, 63, 4], ["void implicit_convolve_sgemm<float, float, 1024, 5, 5, 3, 3, 3, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 6, 848, 141, 142, 140], ["volta_scudnn_128x64_stridedB_small_nn_v1", 7, 666, 95, 96, 94], ["cask_cudnn::computeOffsetsKernel(cask_cudnn::ComputeOffsetsParams)", 131, 330, 3, 4, 2], ["void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<long> >, at::detail::Array<char*, 2>)", 318, 325, 1, 2, 1], ["void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float>, unsigned int, float, 4>)", 6, 256, 43, 43, 42], ["cask_cudnn::computeWgradSplitKOffsetsKernel(cask_cudnn::ComputeSplitKOffsetsParams)", 154, 198, 1, 2, 1], ["cask_cudnn::computeWgradBOffsetsKernel(cask_cudnn::ComputeWgradBOffsetsParams)", 154, 174, 1, 2, 1], ["volta_sgemm_64x32_sliced1x4_nn", 6, 166, 28, 28, 27], ["void at::native::unrolled_elementwise_kernel<at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, OffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast>(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, OffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast)", 6, 162, 27, 27, 27], ["volta_sgemm_64x32_sliced1x4_tn", 6, 145, 24, 25, 24], ["void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)", 39, 135, 3, 5, 3], ["volta_sgemm_128x32_nt", 6, 117, 20, 20, 19], ["cask_cudnn::computeBOffsetsKernel(cask_cudnn::ComputeBOffsetsParams)", 83, 90, 1, 2, 1], ["void (anonymous namespace)::softmax_warp_backward<float, float, float, 10, true>(float*, float const*, float const*, int, int, int)", 6, 64, 11, 11, 10], ["void (anonymous namespace)::softmax_warp_forward<float, float, float, 10, true>(float*, float const*, int, int, int)", 6, 60, 10, 10, 10], ["void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)", 6, 48, 8, 8, 8], ["void splitKreduce_kernel<float, float, float>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*)", 12, 35, 3, 4, 2], ["void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithoutCast, at::detail::Array<char*, 2>::StoreWithoutCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithoutCast, at::detail::Array<char*, 2>::StoreWithoutCast)", 6, 33, 6, 6, 5], ["void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)", 6, 20, 3, 4, 3], ["void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)", 6, 12, 2, 2, 2]]}}
{"total": {"columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["void cudnn::detail::dgrad_engine<float, 512, 6, 5, 3, 3, 3, false>(int, int, int, float const*, int, float const*, int, float*, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)", 86835.0], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", 61395.0], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 6, 7, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 53577.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)", 47050.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)", 41190.0], ["void implicit_convolve_sgemm<float, float, 1024, 6, 7, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 40341.0], ["void implicit_convolve_sgemm<float, float, 128, 6, 7, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 28063.0], ["volta_scudnn_128x128_stridedB_splitK_medium_nn_v1", 27624.0], ["volta_scudnn_128x64_stridedB_splitK_xregs_large_nn_v1", 27184.0], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, 512, true, 1>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", 26234.0], ["volta_sgemm_128x64_nt", 23737.0], ["volta_scudnn_128x128_stridedB_splitK_small_nn_v1", 21753.0], ["volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1", 14259.0], ["volta_sgemm_128x64_nn", 11407.0], ["volta_scudnn_128x64_stridedB_interior_nn_v1", 10904.0], ["void implicit_convolve_sgemm<float, float, 512, 6, 8, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 8816.0], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 5, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 8346.0], ["volta_scudnn_128x64_relu_interior_nn_v1", 7210.0], ["void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float, cudnn::reduced_divisor, int, cudnn::reduced_divisor, cudnn::bnFwPersistentState*, int, float, float, float, int, float, float, cudnnStatus_t*, bool)", 7058.0], ["volta_scudnn_128x128_stridedB_interior_nn_v1", 5717.0], ["void cudnn::ops::scalePackedTensor_kernel<float, float>(cudnnTensor4dStruct, float*, float)", 5482.0], ["void implicit_convolve_sgemm<float, float, 128, 5, 5, 3, 3, 3, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 5341.0], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 6, 8, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 5298.0], ["void cudnn::winograd_nonfused::winogradForwardOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradOutputParams<float, float>)", 5250.0], ["void explicit_convolve_sgemm<float, int, 1024, 5, 5, 3, 3, 3, 0, false>(int, int, int, float const*, int, float const*, int, float*, kernel_conv_params, unsigned long long, int, unsigned long long, int, float, float, int, float const*, float const*)", 4895.0], ["void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(int, float const*, long const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*)", 4781.0], ["void cudnn::winograd_nonfused::winogradForwardData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 4765.0], ["volta_scudnn_128x128_stridedB_medium_nn_v1", 4312.0], ["volta_scudnn_128x64_relu_medium_nn_v1", 3829.0], ["volta_scudnn_128x128_stridedB_small_nn_v1", 3709.0], ["void explicit_convolve_sgemm<float, int, 128, 6, 7, 3, 3, 5, 0, false>(int, int, int, float const*, int, float const*, int, float*, kernel_conv_params, unsigned long long, int, unsigned long long, int, float, float, int, float const*, float const*)", 3533.0], ["volta_scudnn_128x64_relu_xregs_large_nn_v1", 3532.0], ["volta_scudnn_128x64_relu_small_nn_v1", 3395.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)", 2779.0], ["void cudnn::winograd_nonfused::winogradForwardFilter4x4<float, float>(cudnn::winograd_nonfused::WinogradFilterParams<float, float>)", 2617.0], ["void cudnn::winograd_nonfused::winogradWgradData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 2602.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)", 2572.0], ["void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, float>(cudnn::winograd_nonfused::WinogradDeltaParams<float, float>)", 2449.0], ["void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float, cudnn::reduced_divisor, int, cudnn::reduced_divisor, cudnn::bnBwPersistentState*, int, float, float, float, int, float, cudnnStatus_t*, bool)", 2283.0], ["void cudnn::cnn::im2col4d_kernel<float, long>(cudnn::cnn::im2col4d_params, cudnnConvolutionStruct, cudnnTensor4dStruct, float const*, float*)", 1699.0], ["void at::native::(anonymous namespace)::max_pool_forward_nchw<float, float>(int, float const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*, long*)", 1341.0], ["void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradWgradOutputParams<float, float>)", 1306.0], ["void implicit_convolve_sgemm<float, float, 1024, 5, 5, 3, 3, 3, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 848.0], ["volta_scudnn_128x64_stridedB_small_nn_v1", 666.0], ["cask_cudnn::computeOffsetsKernel(cask_cudnn::ComputeOffsetsParams)", 330.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<long> >, at::detail::Array<char*, 2>)", 325.0], ["void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float>, unsigned int, float, 4>)", 256.0], ["cask_cudnn::computeWgradSplitKOffsetsKernel(cask_cudnn::ComputeSplitKOffsetsParams)", 198.0], ["cask_cudnn::computeWgradBOffsetsKernel(cask_cudnn::ComputeWgradBOffsetsParams)", 174.0], ["volta_sgemm_64x32_sliced1x4_nn", 166.0], ["void at::native::unrolled_elementwise_kernel<at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, OffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast>(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, OffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast)", 162.0], ["volta_sgemm_64x32_sliced1x4_tn", 145.0], ["void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)", 135.0], ["volta_sgemm_128x32_nt", 117.0], ["cask_cudnn::computeBOffsetsKernel(cask_cudnn::ComputeBOffsetsParams)", 90.0], ["void (anonymous namespace)::softmax_warp_backward<float, float, float, 10, true>(float*, float const*, float const*, int, int, int)", 64.0], ["void (anonymous namespace)::softmax_warp_forward<float, float, float, 10, true>(float*, float const*, int, int, int)", 60.0], ["void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)", 48.0], ["void splitKreduce_kernel<float, float, float>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*)", 35.0], ["void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithoutCast, at::detail::Array<char*, 2>::StoreWithoutCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithoutCast, at::detail::Array<char*, 2>::StoreWithoutCast)", 33.0], ["void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)", 20.0], ["void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)", 12.0]]}}
{"steps": {"columns": [{"type": "string", "name": "Step"}, {"type": "number", "name": "Kernel"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memcpy"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Memset"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Runtime"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "DataLoader"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "CPU Exec"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}, {"type": "number", "name": "Other"}, {"type": "string", "role": "tooltip", "p": {"html": "true"}}], "rows": [["5", 134069, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 151728us<br><b>Kernel: 134069us</b><br>Percentage: 88.36%</div>", 3344, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 151728us<br><b>Memcpy: 3344us</b><br>Percentage: 2.2%</div>", 76, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 151728us<br><b>Memset: 76us</b><br>Percentage: 0.05%</div>", 3342, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 151728us<br><b>Runtime: 3342us</b><br>Percentage: 2.2%</div>", 58, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 151728us<br><b>DataLoader: 58us</b><br>Percentage: 0.04%</div>", 10091, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 151728us<br><b>CPU Exec: 10091us</b><br>Percentage: 6.65%</div>", 748, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 5<br>Total: 151728us<br><b>Other: 748us</b><br>Percentage: 0.49%</div>"], ["6", 102836, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 118121us<br><b>Kernel: 102836us</b><br>Percentage: 87.06%</div>", 3239, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 118121us<br><b>Memcpy: 3239us</b><br>Percentage: 2.74%</div>", 55, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 118121us<br><b>Memset: 55us</b><br>Percentage: 0.05%</div>", 2859, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 118121us<br><b>Runtime: 2859us</b><br>Percentage: 2.42%</div>", 3, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 118121us<br><b>DataLoader: 3us</b><br>Percentage: 0.0%</div>", 8388, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 118121us<br><b>CPU Exec: 8388us</b><br>Percentage: 7.1%</div>", 741, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 6<br>Total: 118121us<br><b>Other: 741us</b><br>Percentage: 0.63%</div>"], ["7", 95053, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 108411us<br><b>Kernel: 95053us</b><br>Percentage: 87.68%</div>", 3218, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 108411us<br><b>Memcpy: 3218us</b><br>Percentage: 2.97%</div>", 50, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 108411us<br><b>Memset: 50us</b><br>Percentage: 0.05%</div>", 2518, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 108411us<br><b>Runtime: 2518us</b><br>Percentage: 2.32%</div>", 15, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 108411us<br><b>DataLoader: 15us</b><br>Percentage: 0.01%</div>", 6911, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 108411us<br><b>CPU Exec: 6911us</b><br>Percentage: 6.37%</div>", 646, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 7<br>Total: 108411us<br><b>Other: 646us</b><br>Percentage: 0.6%</div>"], ["8", 105509, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 124117us<br><b>Kernel: 105509us</b><br>Percentage: 85.01%</div>", 3251, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 124117us<br><b>Memcpy: 3251us</b><br>Percentage: 2.62%</div>", 57, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 124117us<br><b>Memset: 57us</b><br>Percentage: 0.05%</div>", 2965, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 124117us<br><b>Runtime: 2965us</b><br>Percentage: 2.39%</div>", 23, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 124117us<br><b>DataLoader: 23us</b><br>Percentage: 0.02%</div>", 11606, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 124117us<br><b>CPU Exec: 11606us</b><br>Percentage: 9.35%</div>", 706, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 8<br>Total: 124117us<br><b>Other: 706us</b><br>Percentage: 0.57%</div>"], ["9", 98896, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 115035us<br><b>Kernel: 98896us</b><br>Percentage: 85.97%</div>", 3308, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 115035us<br><b>Memcpy: 3308us</b><br>Percentage: 2.88%</div>", 53, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 115035us<br><b>Memset: 53us</b><br>Percentage: 0.05%</div>", 2980, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 115035us<br><b>Runtime: 2980us</b><br>Percentage: 2.59%</div>", 13, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 115035us<br><b>DataLoader: 13us</b><br>Percentage: 0.01%</div>", 9025, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 115035us<br><b>CPU Exec: 9025us</b><br>Percentage: 7.85%</div>", 760, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 9<br>Total: 115035us<br><b>Other: 760us</b><br>Percentage: 0.66%</div>"], ["10", 113149, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 152072us<br><b>Kernel: 113149us</b><br>Percentage: 74.4%</div>", 3361, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 152072us<br><b>Memcpy: 3361us</b><br>Percentage: 2.21%</div>", 60, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 152072us<br><b>Memset: 60us</b><br>Percentage: 0.04%</div>", 2897, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 152072us<br><b>Runtime: 2897us</b><br>Percentage: 1.91%</div>", 16, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 152072us<br><b>DataLoader: 16us</b><br>Percentage: 0.01%</div>", 8527, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 152072us<br><b>CPU Exec: 8527us</b><br>Percentage: 5.61%</div>", 24062, "<div class=\"visualization-tooltip\" style=\"white-space: nowrap;\">Step 10<br>Total: 152072us<br><b>Other: 24062us</b><br>Percentage: 15.82%</div>"]]}, "performance": [{"name": "Average Step Time", "description": "", "value": 128247, "extra": 100, "children": [{"name": "Kernel", "description": "", "value": 108252, "extra": 84.41}, {"name": "Memcpy", "description": "", "value": 3287, "extra": 2.56}, {"name": "Memset", "description": "", "value": 58, "extra": 0.05}, {"name": "Runtime", "description": "", "value": 2927, "extra": 2.28}, {"name": "DataLoader", "description": "", "value": 21, "extra": 0.02}, {"name": "CPU Exec", "description": "", "value": 9091, "extra": 7.09}, {"name": "Other", "description": "", "value": 4610, "extra": 3.6}]}], "recommendations": "<ul><li>N/A</li></ul>", "environments": [{"title": "Number of Worker(s)", "value": "1"}, {"title": "Device Type", "value": "GPU"}, {"title": "Number of Device(s)", "value": "1"}]}
{"device_total_time": {"title": "Device Total Time", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::cudnn_convolution_backward", 288342], ["CudnnConvolutionBackward", 288342], ["aten::cudnn_convolution_backward_weight", 151977], ["aten::cudnn_convolution_backward_input", 136365], ["aten::cudnn_convolution", 134544], ["aten::_convolution", 134544], ["aten::convolution", 134544], ["aten::conv2d", 134544], ["aten::cudnn_batch_norm_backward", 56960], ["CudnnBatchNormBackward", 56960], ["aten::cudnn_batch_norm", 33334], ["aten::_batch_norm_impl_index", 33334], ["aten::batch_norm", 33334], ["aten::threshold_backward", 26280], ["ReluBackward1", 26280], ["aten::add_", 23354], ["aten::to", 19721], ["aten::copy_", 19721], ["aten::threshold_", 17770], ["aten::relu_", 17770], ["aten::max_pool2d_with_indices_backward", 5053], ["MaxPool2DWithIndicesBackward", 5053], ["torch::autograd::AccumulateGrad", 2918], ["aten::fill_", 2376], ["aten::mul_", 2376], ["aten::zero_", 2370], ["aten::max_pool2d_with_indices", 1341], ["aten::max_pool2d", 1341], ["aten::zeros_like", 948], ["aten::add", 327], ["aten::mm", 288], ["AddmmBackward", 288], ["aten::mean", 258], ["aten::adaptive_avg_pool2d", 258], ["aten::addmm", 204], ["aten::div", 161], ["MeanBackward1", 161], ["aten::_log_softmax_backward_data", 63], ["LogSoftmaxBackward", 63], ["aten::_log_softmax", 60], ["aten::log_softmax", 60], ["aten::nll_loss_forward", 21], ["aten::nll_loss", 21], ["aten::nll_loss_backward", 19], ["NllLossBackward", 19], ["aten::ones_like", 6]]}, "device_self_time": {"title": "Device Self Time", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::cudnn_convolution_backward_weight", 151977], ["aten::cudnn_convolution_backward_input", 136365], ["aten::cudnn_convolution", 134544], ["aten::cudnn_batch_norm_backward", 56960], ["aten::cudnn_batch_norm", 33334], ["aten::threshold_backward", 26280], ["aten::add_", 23354], ["aten::copy_", 19721], ["aten::threshold_", 17770], ["aten::max_pool2d_with_indices_backward", 4105], ["aten::fill_", 2376], ["aten::mul_", 2376], ["aten::max_pool2d_with_indices", 1341], ["aten::add", 327], ["aten::mm", 288], ["aten::mean", 258], ["aten::addmm", 204], ["aten::div", 161], ["aten::_log_softmax_backward_data", 63], ["aten::_log_softmax", 60], ["aten::nll_loss_forward", 21], ["aten::nll_loss_backward", 19]]}, "host_total_time": {"title": "Host Total Time", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::to", 95877], ["aten::copy_", 95330], ["CudnnConvolutionBackward", 89870], ["aten::add_", 88364], ["aten::cudnn_convolution_backward", 85929], ["aten::conv2d", 60800], ["aten::convolution", 56995], ["aten::batch_norm", 53643], ["aten::_convolution", 53318], ["aten::_batch_norm_impl_index", 50036], ["aten::cudnn_convolution", 48860], ["aten::cudnn_batch_norm", 46680], ["torch::autograd::AccumulateGrad", 43241], ["aten::cudnn_convolution_backward_input", 39025], ["aten::cudnn_convolution_backward_weight", 37464], ["CudnnBatchNormBackward", 34153], ["aten::mul_", 32585], ["aten::zero_", 32441], ["aten::cudnn_batch_norm_backward", 29705], ["aten::empty", 29598], ["aten::fill_", 19488], ["aten::relu_", 16391], ["ReluBackward1", 15546], ["aten::add", 14077], ["aten::threshold_backward", 13019], ["aten::threshold_", 8921], ["aten::empty_like", 6343], ["aten::resize_", 3854], ["aten::view", 2824], ["AddmmBackward", 1508], ["aten::addmm", 1219], ["aten::mm", 857], ["MaxPool2DWithIndicesBackward", 779], ["NllLossBackward", 737], ["aten::t", 714], ["aten::max_pool2d", 709], ["aten::max_pool2d_with_indices_backward", 699], ["aten::zeros", 625], ["aten::max_pool2d_with_indices", 622], ["MeanBackward1", 615], ["aten::nll_loss_backward", 604], ["aten::adaptive_avg_pool2d", 530], ["aten::log_softmax", 513], ["aten::nll_loss", 504], ["LogSoftmaxBackward", 454], ["aten::mean", 449], ["aten::_log_softmax", 434], ["aten::nll_loss_forward", 430], ["aten::div", 426], ["aten::_log_softmax_backward_data", 383], ["aten::ones_like", 381], ["AddBackward0", 337], ["aten::transpose", 331], ["aten::zeros_like", 331], ["aten::empty_strided", 319], ["aten::reshape", 223], ["aten::flatten", 187], ["TBackward", 174], ["aten::expand", 150], ["ViewBackward", 130], ["aten::as_strided", 128], ["aten::set_", 118], ["aten::detach_", 95], ["aten::resize_as_", 60], ["aten::conj", 53], ["detach_", 32]]}, "host_self_time": {"title": "Host Self Time", "columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["aten::copy_", 95330], ["aten::add_", 88364], ["aten::cudnn_convolution", 43392], ["aten::cudnn_convolution_backward_input", 34542], ["aten::mul_", 32585], ["aten::cudnn_convolution_backward_weight", 32463], ["aten::cudnn_batch_norm", 32170], ["aten::empty", 29598], ["aten::cudnn_batch_norm_backward", 22529], ["aten::fill_", 19488], ["aten::add", 14077], ["aten::zero_", 13103], ["torch::autograd::AccumulateGrad", 12619], ["aten::threshold_backward", 11098], ["aten::cudnn_convolution_backward", 9440], ["aten::threshold_", 8921], ["aten::relu_", 7470], ["aten::_convolution", 4458], ["CudnnBatchNormBackward", 4448], ["CudnnConvolutionBackward", 3941], ["aten::resize_", 3854], ["aten::conv2d", 3805], ["aten::convolution", 3677], ["aten::batch_norm", 3607], ["aten::empty_like", 3452], ["aten::_batch_norm_impl_index", 3356], ["aten::view", 2824], ["ReluBackward1", 2527], ["aten::addmm", 1079], ["aten::mm", 778], ["aten::nll_loss_backward", 604], ["aten::max_pool2d_with_indices", 482], ["aten::zeros", 465], ["aten::nll_loss_forward", 430], ["aten::mean", 401], ["aten::div", 384], ["aten::t", 383], ["aten::_log_softmax", 340], ["AddBackward0", 337], ["aten::empty_strided", 319], ["aten::max_pool2d_with_indices_backward", 308], ["aten::to", 300], ["aten::_log_softmax_backward_data", 262], ["aten::transpose", 233], ["AddmmBackward", 213], ["NllLossBackward", 133], ["aten::as_strided", 128], ["aten::expand", 120], ["aten::set_", 118], ["aten::max_pool2d", 87], ["MeanBackward1", 87], ["aten::ones_like", 85], ["aten::adaptive_avg_pool2d", 81], ["MaxPool2DWithIndicesBackward", 80], ["aten::log_softmax", 79], ["aten::nll_loss", 74], ["LogSoftmaxBackward", 71], ["aten::reshape", 70], ["aten::detach_", 63], ["aten::flatten", 59], ["aten::zeros_like", 54], ["aten::conj", 53], ["aten::resize_as_", 49], ["TBackward", 43], ["ViewBackward", 35], ["detach_", 32]]}}
{"data": {"columns": [{"type": "string", "name": "Name"}, {"type": "number", "name": "Calls"}, {"type": "number", "name": "Device Self Duration (us)"}, {"type": "number", "name": "Device Total Duration (us)"}, {"type": "number", "name": "Host Self Duration (us)"}, {"type": "number", "name": "Host Total Duration (us)"}], "rows": [["aten::cudnn_convolution_backward_weight", 318, 151977, 151977, 32463, 37464], ["aten::cudnn_convolution_backward_input", 312, 136365, 136365, 34542, 39025], ["aten::cudnn_convolution", 318, 134544, 134544, 43392, 48860], ["aten::cudnn_batch_norm_backward", 318, 56960, 56960, 22529, 29705], ["aten::cudnn_batch_norm", 318, 33334, 33334, 32170, 46680], ["aten::threshold_backward", 294, 26280, 26280, 11098, 13019], ["aten::add_", 2994, 23354, 23354, 88364, 88364], ["aten::copy_", 12, 19721, 19721, 95330, 95330], ["aten::threshold_", 294, 17770, 17770, 8921, 8921], ["aten::max_pool2d_with_indices_backward", 6, 4105, 5053, 308, 699], ["aten::fill_", 978, 2376, 2376, 19488, 19488], ["aten::mul_", 966, 2376, 2376, 32585, 32585], ["aten::max_pool2d_with_indices", 6, 1341, 1341, 482, 622], ["aten::add", 318, 327, 327, 14077, 14077], ["aten::mm", 12, 288, 288, 778, 857], ["aten::mean", 6, 258, 258, 401, 449], ["aten::addmm", 6, 204, 204, 1079, 1219], ["aten::div", 6, 161, 161, 384, 426], ["aten::_log_softmax_backward_data", 6, 63, 63, 262, 383], ["aten::_log_softmax", 6, 60, 60, 340, 434], ["aten::nll_loss_forward", 6, 21, 21, 430, 430], ["aten::nll_loss_backward", 6, 19, 19, 604, 604], ["aten::empty", 5172, 0, 0, 29598, 29598], ["aten::zero_", 996, 0, 2370, 13103, 32441], ["aten::zeros", 24, 0, 0, 465, 625], ["aten::to", 30, 0, 19721, 300, 95877], ["detach_", 12, 0, 0, 32, 32], ["aten::detach_", 12, 0, 0, 63, 95], ["aten::set_", 12, 0, 0, 118, 118], ["aten::empty_strided", 18, 0, 0, 319, 319], ["aten::resize_", 1920, 0, 0, 3854, 3854], ["aten::_convolution", 318, 0, 134544, 4458, 53318], ["aten::convolution", 318, 0, 134544, 3677, 56995], ["aten::conv2d", 318, 0, 134544, 3805, 60800], ["aten::empty_like", 342, 0, 0, 3452, 6343], ["aten::view", 648, 0, 0, 2824, 2824], ["aten::_batch_norm_impl_index", 318, 0, 33334, 3356, 50036], ["aten::batch_norm", 318, 0, 33334, 3607, 53643], ["aten::relu_", 294, 0, 17770, 7470, 16391], ["aten::max_pool2d", 6, 0, 1341, 87, 709], ["aten::adaptive_avg_pool2d", 6, 0, 258, 81, 530], ["aten::reshape", 12, 0, 0, 70, 223], ["aten::flatten", 6, 0, 0, 59, 187], ["aten::as_strided", 42, 0, 0, 128, 128], ["aten::transpose", 30, 0, 0, 233, 331], ["aten::t", 30, 0, 0, 383, 714], ["aten::expand", 12, 0, 0, 120, 150], ["aten::log_softmax", 6, 0, 60, 79, 513], ["aten::nll_loss", 6, 0, 21, 74, 504], ["aten::ones_like", 6, 0, 6, 85, 381], ["NllLossBackward", 6, 0, 19, 133, 737], ["LogSoftmaxBackward", 6, 0, 63, 71, 454], ["aten::conj", 12, 0, 0, 53, 53], ["AddmmBackward", 6, 0, 288, 213, 1508], ["torch::autograd::AccumulateGrad", 966, 0, 2918, 12619, 43241], ["TBackward", 6, 0, 0, 43, 174], ["ViewBackward", 6, 0, 0, 35, 130], ["MeanBackward1", 6, 0, 161, 87, 615], ["ReluBackward1", 294, 0, 26280, 2527, 15546], ["AddBackward0", 96, 0, 0, 337, 337], ["CudnnBatchNormBackward", 318, 0, 56960, 4448, 34153], ["aten::cudnn_convolution_backward", 318, 0, 288342, 9440, 85929], ["CudnnConvolutionBackward", 318, 0, 288342, 3941, 89870], ["aten::zeros_like", 6, 0, 948, 54, 331], ["aten::resize_as_", 6, 0, 0, 49, 60], ["MaxPool2DWithIndicesBackward", 6, 0, 5053, 80, 779]]}}
{"data": {"columns": [{"type": "string", "name": "Name"}, {"type": "number", "name": "Calls"}, {"type": "number", "name": "Total Duration (us)"}, {"type": "number", "name": "Mean Duration (us)"}, {"type": "number", "name": "Max Duration (us)"}, {"type": "number", "name": "Min Duration (us)"}], "rows": [["void cudnn::detail::dgrad_engine<float, 512, 6, 5, 3, 3, 3, false>(int, int, int, float const*, int, float const*, int, float*, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)", 178, 90214, 507, 1092, 154], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 6, 7, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 179, 85427, 477, 814, 381], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", 292, 62347, 214, 802, 43], ["void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)", 613, 47476, 77, 364, 6], ["void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)", 3506, 41486, 12, 364, 1], ["void implicit_convolve_sgemm<float, float, 1024, 6, 7, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 84, 36210, 431, 753, 384], ["void implicit_convolve_sgemm<float, float, 128, 6, 7, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 66, 32761, 496, 854, 362], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, 512, true, 1>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", 150, 26289, 175, 433, 50], ["volta_sgemm_128x64_nt", 126, 23803, 189, 205, 156], ["volta_scudnn_128x128_stridedB_splitK_small_nn_v1", 49, 22477, 459, 705, 329], ["void cudnn::cnn::wgrad_alg0_engine<float, 512, 6, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 21, 18728, 892, 913, 881], ["volta_sgemm_128x64_nn", 78, 14530, 186, 207, 157], ["volta_scudnn_128x64_stridedB_interior_nn_v1", 35, 11515, 329, 528, 260], ["volta_scudnn_128x64_relu_interior_nn_v1", 30, 10280, 343, 530, 295], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 5, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 14, 8372, 598, 993, 207], ["volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1", 21, 7728, 368, 377, 365], ["void cudnn::winograd_nonfused::winogradForwardOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradOutputParams<float, float>)", 141, 7213, 51, 115, 20], ["void cudnn::winograd_nonfused::winogradForwardData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 141, 7133, 51, 143, 17], ["void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float, cudnn::reduced_divisor, int, cudnn::reduced_divisor, cudnn::bnFwPersistentState*, int, float, float, float, int, float, float, cudnnStatus_t*, bool)", 168, 7045, 42, 87, 14], ["void cudnn::ops::scalePackedTensor_kernel<float, float>(cudnnTensor4dStruct, float*, float)", 178, 5920, 33, 158, 6], ["volta_scudnn_128x128_stridedB_interior_nn_v1", 21, 5763, 274, 294, 269], ["void implicit_convolve_sgemm<float, float, 128, 5, 5, 3, 3, 3, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 12, 5352, 446, 449, 443], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 6, 8, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 7, 5335, 762, 781, 744], ["void explicit_convolve_sgemm<float, int, 1024, 5, 5, 3, 3, 3, 0, false>(int, int, int, float const*, int, float const*, int, float*, kernel_conv_params, unsigned long long, int, unsigned long long, int, float, float, int, float const*, float const*)", 6, 4882, 814, 817, 811], ["void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(int, float const*, long const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*)", 7, 4795, 685, 690, 683], ["volta_scudnn_128x128_stridedB_splitK_medium_nn_v1", 7, 4662, 666, 669, 658], ["volta_scudnn_128x128_stridedB_medium_nn_v1", 14, 4373, 312, 327, 297], ["void implicit_convolve_sgemm<float, float, 512, 6, 8, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 6, 4023, 670, 676, 663], ["volta_scudnn_128x64_stridedB_splitK_xregs_large_nn_v1", 6, 4007, 668, 672, 664], ["volta_scudnn_128x64_relu_medium_nn_v1", 6, 3850, 642, 644, 638], ["volta_scudnn_128x64_relu_xregs_large_nn_v1", 6, 3731, 622, 672, 571], ["volta_scudnn_128x128_stridedB_small_nn_v1", 7, 3726, 532, 542, 526], ["volta_scudnn_128x64_relu_small_nn_v1", 12, 3427, 286, 295, 272], ["volta_scudnn_128x128_relu_interior_nn_v1", 6, 3340, 557, 562, 551], ["void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)", 1127, 2771, 2, 24, 1], ["void cudnn::winograd_nonfused::winogradForwardFilter4x4<float, float>(cudnn::winograd_nonfused::WinogradFilterParams<float, float>)", 141, 2699, 19, 66, 3], ["void cudnn::winograd_nonfused::winogradWgradData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 63, 2540, 40, 60, 20], ["void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)", 979, 2534, 3, 158, 0], ["void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, float>(cudnn::winograd_nonfused::WinogradDeltaParams<float, float>)", 63, 2479, 39, 60, 18], ["void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float, cudnn::reduced_divisor, int, cudnn::reduced_divisor, cudnn::bnBwPersistentState*, int, float, float, float, int, float, cudnnStatus_t*, bool)", 54, 2313, 43, 74, 19], ["void at::native::(anonymous namespace)::max_pool_forward_nchw<float, float>(int, float const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*, long*)", 6, 1341, 224, 224, 223], ["void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradWgradOutputParams<float, float>)", 63, 1320, 21, 65, 4], ["void implicit_convolve_sgemm<float, float, 1024, 5, 5, 3, 3, 3, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 6, 864, 144, 147, 142], ["void cudnn::cnn::im2col4d_kernel<float, long>(cudnn::cnn::im2col4d_params, cudnnConvolutionStruct, cudnnTensor4dStruct, float const*, float*)", 6, 601, 100, 101, 99], ["cask_cudnn::computeOffsetsKernel(cask_cudnn::ComputeOffsetsParams)", 137, 352, 3, 6, 2], ["void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<long> >, at::detail::Array<char*, 2>)", 318, 327, 1, 2, 1], ["void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float>, unsigned int, float, 4>)", 6, 258, 43, 43, 43], ["volta_sgemm_64x32_sliced1x4_nn", 6, 161, 27, 27, 26], ["void at::native::unrolled_elementwise_kernel<at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, OffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast>(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, OffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast)", 6, 161, 27, 27, 26], ["volta_sgemm_64x32_sliced1x4_tn", 6, 144, 24, 24, 24], ["volta_sgemm_128x32_nt", 6, 115, 19, 20, 19], ["cask_cudnn::computeWgradBOffsetsKernel(cask_cudnn::ComputeWgradBOffsetsParams)", 62, 87, 1, 2, 1], ["cask_cudnn::computeBOffsetsKernel(cask_cudnn::ComputeBOffsetsParams)", 77, 85, 1, 2, 1], ["void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)", 21, 84, 4, 4, 4], ["cask_cudnn::computeWgradSplitKOffsetsKernel(cask_cudnn::ComputeSplitKOffsetsParams)", 62, 75, 1, 2, 1], ["void (anonymous namespace)::softmax_warp_backward<float, float, float, 10, true>(float*, float const*, float const*, int, int, int)", 6, 63, 10, 11, 10], ["void (anonymous namespace)::softmax_warp_forward<float, float, float, 10, true>(float*, float const*, int, int, int)", 6, 60, 10, 10, 10], ["void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)", 6, 48, 8, 8, 8], ["void splitKreduce_kernel<float, float, float>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*)", 12, 36, 3, 4, 2], ["void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithoutCast, at::detail::Array<char*, 2>::StoreWithoutCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithoutCast, at::detail::Array<char*, 2>::StoreWithoutCast)", 6, 36, 6, 6, 6], ["void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)", 6, 21, 4, 4, 3], ["void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)", 6, 13, 2, 3, 2]]}}
{"total": {"columns": [{"type": "string", "name": "name"}, {"type": "number", "name": "value"}], "rows": [["void cudnn::detail::dgrad_engine<float, 512, 6, 5, 3, 3, 3, false>(int, int, int, float const*, int, float const*, int, float*, kernel_grad_params, unsigned long long, int, unsigned long long, int, float, int, int, int)", 90214.0], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 6, 7, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 85427.0], ["void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)", 62347.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)", 47476.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)", 41486.0], ["void implicit_convolve_sgemm<float, float, 1024, 6, 7, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 36210.0], ["void implicit_convolve_sgemm<float, float, 128, 6, 7, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 32761.0], ["void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, 512, true, 1>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)", 26289.0], ["volta_sgemm_128x64_nt", 23803.0], ["volta_scudnn_128x128_stridedB_splitK_small_nn_v1", 22477.0], ["void cudnn::cnn::wgrad_alg0_engine<float, 512, 6, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 18728.0], ["volta_sgemm_128x64_nn", 14530.0], ["volta_scudnn_128x64_stridedB_interior_nn_v1", 11515.0], ["volta_scudnn_128x64_relu_interior_nn_v1", 10280.0], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 5, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 8372.0], ["volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1", 7728.0], ["void cudnn::winograd_nonfused::winogradForwardOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradOutputParams<float, float>)", 7213.0], ["void cudnn::winograd_nonfused::winogradForwardData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 7133.0], ["void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float, cudnn::reduced_divisor, int, cudnn::reduced_divisor, cudnn::bnFwPersistentState*, int, float, float, float, int, float, float, cudnnStatus_t*, bool)", 7045.0], ["void cudnn::ops::scalePackedTensor_kernel<float, float>(cudnnTensor4dStruct, float*, float)", 5920.0], ["volta_scudnn_128x128_stridedB_interior_nn_v1", 5763.0], ["void implicit_convolve_sgemm<float, float, 128, 5, 5, 3, 3, 3, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 5352.0], ["void cudnn::cnn::wgrad_alg0_engine<float, 128, 6, 8, 3, 3, 5, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)", 5335.0], ["void explicit_convolve_sgemm<float, int, 1024, 5, 5, 3, 3, 3, 0, false>(int, int, int, float const*, int, float const*, int, float*, kernel_conv_params, unsigned long long, int, unsigned long long, int, float, float, int, float const*, float const*)", 4882.0], ["void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(int, float const*, long const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*)", 4795.0], ["volta_scudnn_128x128_stridedB_splitK_medium_nn_v1", 4662.0], ["volta_scudnn_128x128_stridedB_medium_nn_v1", 4373.0], ["void implicit_convolve_sgemm<float, float, 512, 6, 8, 3, 3, 5, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 4023.0], ["volta_scudnn_128x64_stridedB_splitK_xregs_large_nn_v1", 4007.0], ["volta_scudnn_128x64_relu_medium_nn_v1", 3850.0], ["volta_scudnn_128x64_relu_xregs_large_nn_v1", 3731.0], ["volta_scudnn_128x128_stridedB_small_nn_v1", 3726.0], ["volta_scudnn_128x64_relu_small_nn_v1", 3427.0], ["volta_scudnn_128x128_relu_interior_nn_v1", 3340.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)", 2771.0], ["void cudnn::winograd_nonfused::winogradForwardFilter4x4<float, float>(cudnn::winograd_nonfused::WinogradFilterParams<float, float>)", 2699.0], ["void cudnn::winograd_nonfused::winogradWgradData4x4<float, float>(cudnn::winograd_nonfused::WinogradDataParams<float, float>)", 2540.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)", 2534.0], ["void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, float>(cudnn::winograd_nonfused::WinogradDeltaParams<float, float>)", 2479.0], ["void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float, cudnn::reduced_divisor, int, cudnn::reduced_divisor, cudnn::bnBwPersistentState*, int, float, float, float, int, float, cudnnStatus_t*, bool)", 2313.0], ["void at::native::(anonymous namespace)::max_pool_forward_nchw<float, float>(int, float const*, int, int, int, int, int, int, int, int, int, int, int, int, int, int, float*, long*)", 1341.0], ["void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, float>(cudnn::winograd_nonfused::WinogradWgradOutputParams<float, float>)", 1320.0], ["void implicit_convolve_sgemm<float, float, 1024, 5, 5, 3, 3, 3, 1, false, true, true>(int, int, int, float const*, int, float*, float const*, kernel_conv_params, unsigned long long, int, float, float, int, float const*, float const*, bool, int, int)", 864.0], ["void cudnn::cnn::im2col4d_kernel<float, long>(cudnn::cnn::im2col4d_params, cudnnConvolutionStruct, cudnnTensor4dStruct, float const*, float*)", 601.0], ["cask_cudnn::computeOffsetsKernel(cask_cudnn::ComputeOffsetsParams)", 352.0], ["void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<long> >, at::detail::Array<char*, 2>)", 327.0], ["void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float>, unsigned int, float, 4>)", 258.0], ["volta_sgemm_64x32_sliced1x4_nn", 161.0], ["void at::native::unrolled_elementwise_kernel<at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, OffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast>(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, OffsetCalculator<1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast)", 161.0], ["volta_sgemm_64x32_sliced1x4_tn", 144.0], ["volta_sgemm_128x32_nt", 115.0], ["cask_cudnn::computeWgradBOffsetsKernel(cask_cudnn::ComputeWgradBOffsetsParams)", 87.0], ["cask_cudnn::computeBOffsetsKernel(cask_cudnn::ComputeBOffsetsParams)", 85.0], ["void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)", 84.0], ["cask_cudnn::computeWgradSplitKOffsetsKernel(cask_cudnn::ComputeSplitKOffsetsParams)", 75.0], ["void (anonymous namespace)::softmax_warp_backward<float, float, float, 10, true>(float*, float const*, float const*, int, int, int)", 63.0], ["void (anonymous namespace)::softmax_warp_forward<float, float, float, 10, true>(float*, float const*, int, int, int)", 60.0], ["void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)", 48.0], ["void splitKreduce_kernel<float, float, float>(cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*)", 36.0], ["void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithoutCast, at::detail::Array<char*, 2>::StoreWithoutCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#8}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>, OffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithoutCast, at::detail::Array<char*, 2>::StoreWithoutCast)", 36.0], ["void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)", 21.0], ["void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)", 13.0]]}}
