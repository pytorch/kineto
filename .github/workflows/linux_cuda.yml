name: Linux CUDA build and test

on:
  push:
    branches:
      - main
  pull_request:

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}-${{ github.event_name == 'workflow_dispatch' }}
  cancel-in-progress: true

jobs:
  pr-test:
    uses: pytorch/test-infra/.github/workflows/linux_job_v2.yml@main
    with:
      job-name: linux-cuda-build-and-test
      # AWS A10G GPU instance
      runner: linux.g5.4xlarge.nvidia.gpu
      timeout: 180
      # Checkout the Kineto repo at the PR ref with submodules
      repository: ${{ github.repository }}
      ref: ${{ github.ref }}
      submodules: recursive
      gpu-arch-type: cuda
      gpu-arch-version: "12.6"
      script: |
        set -eux

        echo "====: Working directory: $(pwd)"

        # Upgrade pip
        python -m pip install --upgrade pip
        echo "====: Installed pip version: $(python -m pip --version)"

        # Ensure cmake is minimum version Kineto requires
        conda install -y cmake>=3.22
        echo "====: Installed cmake version: $(cmake --version)"

        # Build libkineto (static and shared library)
        mkdir -p build_static build_shared

        pushd build_static
        cmake -DKINETO_LIBRARY_TYPE=static ../libkineto/
        make -j
        popd
        echo "====: Compiled static libkineto"

        pushd build_shared
        cmake -DKINETO_LIBRARY_TYPE=shared ../libkineto/
        make -j
        popd
        echo "====: Compiled shared libkineto"

        # Run libkineto tests
        pushd build_static
        make test
        popd
        echo "====: Ran static libkineto tests"

        # Clone PyTorch and replace its Kineto with PR version
        git clone --recursive --branch viable/strict https://github.com/pytorch/pytorch.git
        echo "====: Cloned PyTorch"

        pushd pytorch
        rm -rf third_party/kineto
        ln -s ../kineto third_party/kineto
        echo "====: Linked PR version of Kineto to PyTorch"

        # Build PyTorch from source
        pip install -r requirements.txt
        export USE_CUDA=1
        export BUILD_TEST=1
        python setup.py develop
        echo "====: Built PyTorch from source"

        # Run PyTorch profiler tests
        # TODO: Dynamically add/remove tests to the exclusion list based on their
        # status on trunk instead of maintaining a hardcoded list of known failures.
        # This will prevent the list from becoming stale as tests get fixed upstream.
        python -m pytest test/profiler/ -v \
          --deselect=test/profiler/test_memory_profiler.py::TestDataFlow::test_data_flow_graph_complicated \
          --deselect=test/profiler/test_memory_profiler.py::TestMemoryProfilerE2E::test_categories_e2e_sequential_fwd_bwd \
          --deselect=test/profiler/test_memory_profiler.py::TestMemoryProfilerE2E::test_categories_e2e_simple_fwd_bwd \
          --deselect=test/profiler/test_memory_profiler.py::TestMemoryProfilerE2E::test_categories_e2e_simple_fwd_bwd_step \
          --deselect=test/profiler/test_profiler.py::TestProfiler::test_kineto \
          --deselect=test/profiler/test_profiler.py::TestProfiler::test_user_annotation \
          --deselect=test/profiler/test_profiler.py::TestExperimentalUtils::test_fuzz_symbolize \
          --deselect=test/profiler/test_profiler.py::TestExperimentalUtils::test_profiler_debug_autotuner \
          --deselect=test/profiler/test_torch_tidy.py::TestTorchTidyProfiler::test_tensorimpl_invalidation_scalar_args
        popd
        echo "====: Ran PyTorch profiler tests"
